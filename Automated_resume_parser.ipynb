{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODCkPgHQsaYMuW/8w2ByHQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3" 
    },
    "language_info": { 
      "name": "python" 
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I am installing all the necessary Python packages needed for this project\n", 
        "# pdfplumber and PyMuPDF are for reading PDF files\n",
        "# python-docx is for reading Word documents\n",
        "# spacy and nltk are for natural language processing (like extracting names, skills)\n",
        "# phonenumbers and email-validator are for validating contact info\n",
        "!pip install pdfplumber PyMuPDF python-docx spacy nltk phonenumbers email-validator\n",
        "\n",
        "# ✅ Downloading the English small model for spaCy\n",
        "# This is used for entity recognition like names, organizations, etc.\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"✅ Installation complete!\")\n" 
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mBNrearHAzh",
        "outputId": "55958444-cb19-4cb4-8b4a-bb77b8527846" 
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting phonenumbers\n",
            "  Downloading phonenumbers-9.0.13-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting email-validator\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Collecting dnspython>=2.0.0 (from email-validator)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator) (3.10)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-9.0.13-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: phonenumbers, python-docx, pypdfium2, PyMuPDF, dnspython, email-validator, pdfminer.six, pdfplumber\n",
            "Successfully installed PyMuPDF-1.26.4 dnspython-2.7.0 email-validator-2.3.0 pdfminer.six-20250506 pdfplumber-0.11.7 phonenumbers-9.0.13 pypdfium2-4.30.0 python-docx-1.2.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "✅ Installation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Importing all libraries used in the project\n",  
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from typing import Dict, List\n", 
        "import warnings\n",
        "\n",
        "# Ignore warnings to keep output clean\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PDF and Word document processing\n",
        "import pdfplumber  # Extract text from PDFs\n",
        "import fitz  # PyMuPDF for PDF extraction\n",
        "import docx  # For extracting text from DOCX files\n",
        "\n",
        "# NLP libraries\n",
        "import spacy  # For extracting entities like names\n",
        "import nltk  # Tokenizing text, removing stopwords\n",
        "\n",
        "# Validation for emails\n",
        "from email_validator import validate_email, EmailNotValidError\n",
        "\n",
        "# Download basic NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load spaCy English model for entity recognition\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"✅ Libraries loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oQ_PYeJHDD5",
        "outputId": "5e20a073-36f5-4a12-d0ca-2e1182beb86d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I have created a class called ResumeParser\n",
        "# This class contains all the functions needed to parse a resume\n",
        "# I am handling PDF, DOCX, extracting name, email, phone, skills, education, experience\n",
        "\n",
        "class ResumeParser:\n",
        "    def __init__(self):\n",
        "        # Load spaCy NLP model\n",
        "        self.nlp = nlp\n",
        "\n",
        "        # List of common technical skills to search in resumes\n",
        "        self.tech_skills = [\n",
        "            'python', 'java', 'javascript', 'c++', 'c#', 'php', 'ruby', 'go', 'swift',\n",
        "            'html', 'css', 'react', 'angular', 'vue', 'node.js', 'django', 'flask',\n",
        "            'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'aws', 'azure', 'docker',\n",
        "            'kubernetes', 'git', 'machine learning', 'tensorflow', 'pytorch', 'pandas'\n",
        "        ]\n",
        "\n",
        "        # List of common soft skills\n",
        "        self.soft_skills = [\n",
        "            'leadership', 'communication', 'teamwork', 'problem solving', 'project management',\n",
        "            'analytical', 'creative', 'adaptable', 'detail-oriented', 'organized'\n",
        "        ]\n",
        "\n",
        "    # ================= PDF and DOCX extraction =================\n",
        "    def extract_text_from_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file using pdfplumber first, then fallback to PyMuPDF\"\"\"\n",
        "        try:\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                text = \"\"\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "                if text.strip():\n",
        "                    return self.clean_text(text)\n",
        "\n",
        "            # Fallback using PyMuPDF\n",
        "            doc = fitz.open(file_path)\n",
        "            text = \"\"\n",
        "            for page_num in range(len(doc)):\n",
        "                page = doc.load_page(page_num)\n",
        "                text += page.get_text() + \"\\n\"\n",
        "            doc.close()\n",
        "            return self.clean_text(text)\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to extract PDF text: {e}\")\n",
        "\n",
        "    def extract_text_from_docx(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from DOCX file\"\"\"\n",
        "        try:\n",
        "            doc = docx.Document(file_path)\n",
        "            text = [p.text for p in doc.paragraphs if p.text.strip()]\n",
        "            return '\\n'.join(text)\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to extract DOCX text: {e}\")\n",
        "\n",
        "    # ================= Cleaning Text =================\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean text by removing extra spaces and newlines\"\"\"\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Remove multiple blank lines\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
        "        return text.strip()\n",
        "\n",
        "    # ================= Contact Info =================\n",
        "    def extract_email(self, text: str) -> str:\n",
        "        \"\"\"Find first valid email in text\"\"\"\n",
        "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "        emails = re.findall(email_pattern, text)\n",
        "        for email in emails:\n",
        "            try:\n",
        "                validate_email(email)  # Validate email format\n",
        "                return email\n",
        "            except:\n",
        "                continue\n",
        "        return \"\"\n",
        "\n",
        "    def extract_phone(self, text: str) -> str:\n",
        "        \"\"\"Find first phone number in text\"\"\"\n",
        "        phone_patterns = [\n",
        "            r'\\+?1?[-.\\s]?\\(?([0-9]{3})\\)?[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})',\n",
        "            r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b'\n",
        "        ]\n",
        "        for pattern in phone_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            if matches:\n",
        "                if isinstance(matches[0], tuple):\n",
        "                    return ''.join(matches[0])\n",
        "                else:\n",
        "                    return matches[0]\n",
        "        return \"\"\n",
        "\n",
        "    # ================= Name Extraction =================\n",
        "    def extract_name(self, text: str) -> str:\n",
        "        \"\"\"Extract candidate name using NLP or fallback pattern matching\"\"\"\n",
        "        doc = self.nlp(text[:500])  # Only check first 500 characters\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ == \"PERSON\" and len(ent.text.split()) >= 2:\n",
        "                return ent.text\n",
        "\n",
        "        # Fallback: check first 5 lines for Name pattern\n",
        "        lines = text.split('\\n')[:5]\n",
        "        for line in lines:\n",
        "            if re.match(r'^[A-Z][a-z]+ [A-Z][a-z]+', line.strip()):\n",
        "                words = line.strip().split()[:2]\n",
        "                if len(words) == 2:\n",
        "                    return ' '.join(words)\n",
        "        return \"\"\n",
        "\n",
        "    # ================= Skills Extraction =================\n",
        "    def extract_skills(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Find both technical and soft skills mentioned in text\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        found_tech = [s.title() for s in self.tech_skills if s in text_lower]\n",
        "        found_soft = [s.title() for s in self.soft_skills if s in text_lower]\n",
        "        return {'technical': found_tech, 'soft': found_soft}\n",
        "\n",
        "    # ================= Education =================\n",
        "    def extract_education(self, text: str) -> Dict:\n",
        "        \"\"\"Extract education details like degree, university, field, year\"\"\"\n",
        "        education = {'degree': '', 'university': '', 'field': '', 'year': ''}\n",
        "\n",
        "        # Degree pattern\n",
        "        degree_patterns = [r'(bachelor|master|phd|b\\.s|m\\.s|b\\.a|m\\.a|b\\.tech|m\\.tech)', r'(diploma|certificate)']\n",
        "        for pattern in degree_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                education['degree'] = match.group(0)\n",
        "                break\n",
        "\n",
        "        # University keywords\n",
        "        uni_keywords = ['university', 'college', 'institute', 'iit', 'nit']\n",
        "        for line in text.split('\\n'):\n",
        "            for keyword in uni_keywords:\n",
        "                if keyword.lower() in line.lower():\n",
        "                    education['university'] = line.strip()\n",
        "                    break\n",
        "\n",
        "        # Graduation year\n",
        "        years = re.findall(r'\\b(19|20)\\d{2}\\b', text)\n",
        "        if years:\n",
        "            education['year'] = max(years)\n",
        "\n",
        "        return education\n",
        "\n",
        "    # ================= Work Experience =================\n",
        "    def extract_experience(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Extract work experience from resume text\"\"\"\n",
        "        experiences = []\n",
        "        lines = text.split('\\n')\n",
        "        in_experience = False\n",
        "        current_exp = {}\n",
        "\n",
        "        for line in lines:\n",
        "            line_lower = line.lower().strip()\n",
        "\n",
        "            # Check if experience section starts\n",
        "            if re.search(r'\\b(experience|employment|work)\\b', line_lower) and len(line) < 30:\n",
        "                in_experience = True\n",
        "                continue\n",
        "\n",
        "            # Stop if experience section ends\n",
        "            if in_experience and re.search(r'\\b(education|skills|projects)\\b', line_lower) and len(line) < 30:\n",
        "                break\n",
        "\n",
        "            # Record experience\n",
        "            if in_experience and line.strip():\n",
        "                if re.search(r'\\b(company|corp|inc|ltd|pvt|llc)\\b', line_lower):\n",
        "                    if current_exp:\n",
        "                        experiences.append(current_exp)\n",
        "                    current_exp = {'company': line.strip(), 'position': '', 'duration': ''}\n",
        "                elif current_exp and not current_exp.get('position'):\n",
        "                    current_exp['position'] = line.strip()\n",
        "\n",
        "        if current_exp:\n",
        "            experiences.append(current_exp)\n",
        "\n",
        "        return experiences\n",
        "\n",
        "    # ================= Main Parsing Function =================\n",
        "    def parse_resume(self, file_path: str) -> Dict:\n",
        "        \"\"\"Parse resume file and return structured data\"\"\"\n",
        "        try:\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                text = self.extract_text_from_pdf(file_path)\n",
        "            elif file_path.lower().endswith(('.docx', '.doc')):\n",
        "                text = self.extract_text_from_docx(file_path)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported file format. Use PDF or DOCX.\")\n",
        "\n",
        "            result = {\n",
        "                'filename': os.path.basename(file_path),\n",
        "                'raw_text': text,\n",
        "                'personal_info': {\n",
        "                    'name': self.extract_name(text),\n",
        "                    'email': self.extract_email(text),\n",
        "                    'phone': self.extract_phone(text)\n",
        "                },\n",
        "                'skills': self.extract_skills(text),\n",
        "                'education': self.extract_education(text),\n",
        "                'experience': self.extract_experience(text),\n",
        "                'parsing_success': True\n",
        "            }\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'filename': os.path.basename(file_path) if file_path else 'unknown', 'error': str(e), 'parsing_success': False}\n",
        "\n",
        "# Initialize parser instance\n",
        "parser = ResumeParser()\n",
        "print(\"✅ Resume Parser initialized and ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lswFTMkXHF99",
        "outputId": "73152f27-dcbd-4f04-e10d-8f87670b2f06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Resume Parser initialized and ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I am allowing the user to upload a resume file (PDF or DOCX)\n",
        "# This can be replaced with an input path if running locally\n",
        "from google.colab import files  # Only for Google Colab\n",
        "\n",
        "# Prompt user to upload a file\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Save uploaded file to local\n",
        "for filename in uploaded_files.keys():\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(uploaded_files[filename])\n",
        "    print(f\"✅ File '{filename}' uploaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "JDtImt-mHIJw",
        "outputId": "1be287cd-4c6f-40fc-99e6-cf7f8b22d300"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d543fe18-462c-492f-9a24-c345ffd6d18f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d543fe18-462c-492f-9a24-c345ffd6d18f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n", 
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Prabhakar_Rayal_Resume.pdf to Prabhakar_Rayal_Resume.pdf\n", 
            "✅ File 'Prabhakar_Rayal_Resume.pdf' uploaded successfully!\n" 
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I am using the ResumeParser instance to parse the uploaded file\n",
        "# The parser will extract name, email, phone, skills, education, and experience\n",
        "for filename in uploaded_files.keys():\n",
        "    result = parser.parse_resume(filename)\n",
        "\n",
        "    # Save parsed result as JSON (optional)\n",
        "    json_filename = filename.split('.')[0] + \"_parsed.json\"\n",
        "    with open(json_filename, 'w') as f:\n",
        "        import json\n",
        "        json.dump(result, f, indent=4)\n",
        "\n",
        "    print(f\"✅ Parsing complete for '{filename}'. Results saved as '{json_filename}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyvqrvUGHLW9",
        "outputId": "e82a7780-8664-4aa2-c36f-fb56060e9517"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parsing complete for 'Prabhakar_Rayal_Resume.pdf'. Results saved as 'Prabhakar_Rayal_Resume_parsed.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I am displaying parsed data in a readable format\n",
        "# This is helpful to quickly check all extracted details\n",
        "\n",
        "import pprint  # Pretty print dictionary\n",
        "\n",
        "for filename in uploaded_files.keys():\n",
        "    print(f\"\\n📄 Parsed Data for '{filename}':\")\n",
        "    pprint.pprint(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXe3ltdoHO7X",
        "outputId": "1925f25d-c6d6-464e-8581-ec6a8f8ce240"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Parsed Data for 'Prabhakar_Rayal_Resume.pdf':\n",
            "{'education': {'degree': 'B.Tech',\n",
            "               'field': '',\n",
            "               'university': 'Prabhakar Rayal Email: '\n",
            "                             'prabhakarrayalarcy@gmail.com • Phone: 90 XXXX XXXX '\n",
            "                             'LinkedIn: '\n",
            "                             'https://www.linkedin.com/in/prabhakar-rayal-663968259/ '\n",
            "                             '• GitHub: https://github.com/Prabhakarrayal '\n",
            "                             'Professional Summary Proactive Computer Science '\n",
            "                             'graduate aspiring for roles as a Software '\n",
            "                             'Engineer, AI/ML Engineer, or Web Developer. '\n",
            "                             'Skilled in building scalable applications and '\n",
            "                             'intelligent systems with expertise in Python, '\n",
            "                             'Flask, JavaScript, and cloud technologies. '\n",
            "                             'Completed hands-on projects in AI, machine '\n",
            "                             'learning, and full-stack web development. Strong '\n",
            "                             'foundation in Data Structures, Algorithms, and '\n",
            "                             'Object-Oriented Programming. Certified in AWS '\n",
            "                             'Cloud Basics. Education B.Tech in Computer '\n",
            "                             'Science | Graphic Era Hill University, Dehradun '\n",
            "                             '(2021 – 2025) | CGPA: 7.59/10 12th Grade (I.S.E) '\n",
            "                             '| Modern School, Rishikesh (2021) | 69.5% Key '\n",
            "                             'Skills Programming Languages: Python, Java, C++, '\n",
            "                             'C, JavaScript, PHP, SQL Web Technologies & '\n",
            "                             'Frameworks: Flask, HTML, CSS, RESTful APIs. '\n",
            "                             'Libraries & Tools: Pandas, NumPy, Scikit-learn, '\n",
            "                             'Tkinter, Git, VS Code, IntelliJ. Databases & '\n",
            "                             'Cloud: MySQL, AWS (Infosys Certified – Cloud '\n",
            "                             'Basics). Software Development: Object-Oriented '\n",
            "                             'Programming (OOP), Agile Methodologies, '\n",
            "                             'Debugging, Testing, Deployment. Other Skills: '\n",
            "                             'Cybersecurity Awareness, Generative AI Tools '\n",
            "                             '(ChatGPT, Gemini), MS Excel, Google Sheets. '\n",
            "                             'Languages English (Professional Working '\n",
            "                             'Proficiency) • Hindi (Native) • Japanese '\n",
            "                             '(Elementary, Learning). Projects Medical Image '\n",
            "                             'Denoising using ML (Major Project) Python, '\n",
            "                             'Autoencoders, Jan 2025 – Jun 2025 Engineered '\n",
            "                             'ML-based denoising models to enhance diagnostic '\n",
            "                             'image clarity. Achieved 35% reduction in noise, '\n",
            "                             'improving visibility for medical analysis. '\n",
            "                             'Medicine Recommendation System + Chatbot Python, '\n",
            "                             'Flask, JS, Feb 2024 – Jul 2024 Designed AI '\n",
            "                             'chatbot with web UI recommending OTC medicines '\n",
            "                             'for 100+ symptoms. Improved accessibility by '\n",
            "                             'providing instant responses with 90% accuracy. '\n",
            "                             'Supermarket Billing System C++, Apr 2023 – Jul '\n",
            "                             '2023 Automated billing and inventory, reducing '\n",
            "                             'manual errors by 70%. Movie Recommender System '\n",
            "                             'Python, Pandas, Nov 2022 – Feb 2023 Developed '\n",
            "                             'similarity-based recommender to generate '\n",
            "                             'personalized movie suggestions. BGMI Tournament '\n",
            "                             'Website Frontend HTML, CSS, JS, Feb 2022 – Apr '\n",
            "                             '2022 Developed a gaming tournament site enabling '\n",
            "                             '200+ registrations for the tournament. '\n",
            "                             'Certifications AWS Cloud Computing Basics – '\n",
            "                             'Infosys Certified (2025). Cybersecurity '\n",
            "                             'Awareness Program – Infotech Certified (2023). '\n",
            "                             'Achievements Finalist – Hackathon 1.0 (Web '\n",
            "                             'Development). Participated in events (C-Quiz, '\n",
            "                             'Treasure Hunt, Fandango). Teamwork & '\n",
            "                             'coordination in multiple group projects and '\n",
            "                             'university fests.',\n",
            "               'year': '20'},\n",
            " 'experience': [],\n",
            " 'filename': 'Prabhakar_Rayal_Resume.pdf',\n",
            " 'parsing_success': True,\n",
            " 'personal_info': {'email': 'prabhakarrayalarcy@gmail.com',\n",
            "                   'name': 'Prabhakar Rayal Email',\n",
            "                   'phone': '90 XXXX XXXX'},\n",
            " 'raw_text': 'Prabhakar Rayal Email: prabhakarrayalarcy@gmail.com • Phone: '\n",
            "             '90 XXXX XXXX LinkedIn: '\n",
            "             'https://www.linkedin.com/in/prabhakar-rayal-663968259/ • GitHub: '\n",
            "             'https://github.com/Prabhakarrayal Professional Summary Proactive '\n",
            "             'Computer Science graduate aspiring for roles as a Software '\n",
            "             'Engineer, AI/ML Engineer, or Web Developer. Skilled in building '\n",
            "             'scalable applications and intelligent systems with expertise in '\n",
            "             'Python, Flask, JavaScript, and cloud technologies. Completed '\n",
            "             'hands-on projects in AI, machine learning, and full-stack web '\n",
            "             'development. Strong foundation in Data Structures, Algorithms, '\n",
            "             'and Object-Oriented Programming. Certified in AWS Cloud Basics. '\n",
            "             'Education B.Tech in Computer Science | Graphic Era Hill '\n",
            "             'University, Dehradun (2021 – 2025) | CGPA: 7.59/10 12th Grade '\n",
            "             '(I.S.E) | Modern School, Rishikesh (2021) | 69.5% Key Skills '\n",
            "             'Programming Languages: Python, Java, C++, C, JavaScript, PHP, '\n",
            "             'SQL Web Technologies & Frameworks: Flask, HTML, CSS, RESTful '\n",
            "             'APIs. Libraries & Tools: Pandas, NumPy, Scikit-learn, Tkinter, '\n",
            "             'Git, VS Code, IntelliJ. Databases & Cloud: MySQL, AWS (Infosys '\n",
            "             'Certified – Cloud Basics). Software Development: Object-Oriented '\n",
            "             'Programming (OOP), Agile Methodologies, Debugging, Testing, '\n",
            "             'Deployment. Other Skills: Cybersecurity Awareness, Generative AI '\n",
            "             'Tools (ChatGPT, Gemini), MS Excel, Google Sheets. Languages '\n",
            "             'English (Professional Working Proficiency) • Hindi (Native) • '\n",
            "             'Japanese (Elementary, Learning). Projects Medical Image '\n",
            "             'Denoising using ML (Major Project) Python, Autoencoders, Jan '\n",
            "             '2025 – Jun 2025 Engineered ML-based denoising models to enhance '\n",
            "             'diagnostic image clarity. Achieved 35% reduction in noise, '\n",
            "             'improving visibility for medical analysis. Medicine '\n",
            "             'Recommendation System + Chatbot Python, Flask, JS, Feb 2024 – '\n",
            "             'Jul 2024 Designed AI chatbot with web UI recommending OTC '\n",
            "             'medicines for 100+ symptoms. Improved accessibility by providing '\n",
            "             'instant responses with 90% accuracy. Supermarket Billing System '\n",
            "             'C++, Apr 2023 – Jul 2023 Automated billing and inventory, '\n",
            "             'reducing manual errors by 70%. Movie Recommender System Python, '\n",
            "             'Pandas, Nov 2022 – Feb 2023 Developed similarity-based '\n",
            "             'recommender to generate personalized movie suggestions. BGMI '\n",
            "             'Tournament Website Frontend HTML, CSS, JS, Feb 2022 – Apr 2022 '\n",
            "             'Developed a gaming tournament site enabling 200+ registrations '\n",
            "             'for the tournament. Certifications AWS Cloud Computing Basics – '\n",
            "             'Infosys Certified (2025). Cybersecurity Awareness Program – '\n",
            "             'Infotech Certified (2023). Achievements Finalist – Hackathon 1.0 '\n",
            "             '(Web Development). Participated in events (C-Quiz, Treasure '\n",
            "             'Hunt, Fandango). Teamwork & coordination in multiple group '\n",
            "             'projects and university fests.',\n",
            " 'skills': {'soft': ['Teamwork'],\n",
            "            'technical': ['Python',\n",
            "                          'Java',\n",
            "                          'Javascript',\n",
            "                          'C++',\n",
            "                          'Php',\n",
            "                          'Go',\n",
            "                          'Html',\n",
            "                          'Css',\n",
            "                          'Flask',\n",
            "                          'Sql',\n",
            "                          'Mysql',\n",
            "                          'Aws',\n",
            "                          'Git',\n",
            "                          'Machine Learning',\n",
            "                          'Pandas']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I am demonstrating how to search the parsed data for specific info\n",
        "# Example: Find if the candidate knows 'Python' or has 'Machine Learning' experience\n",
        "\n",
        "search_skill = 'Python'\n",
        "\n",
        "# Convert both sides to lowercase for consistent matching\n",
        "technical_skills = [s.lower() for s in result['skills']['technical']]\n",
        "if search_skill.lower() in technical_skills:\n",
        "    print(f\"✅ Candidate has technical skill: {search_skill}\")\n",
        "else:\n",
        "    print(f\"❌ Candidate does not have technical skill: {search_skill}\")\n",
        "\n",
        "# 🔎 Search for experience in a specific company\n",
        "company_search = 'Google'\n",
        "companies = [exp.get('company','').lower() for exp in result['experience']]\n",
        "if company_search.lower() in companies:\n",
        "    print(f\"✅ Candidate has worked at {company_search}\")\n",
        "else:\n",
        "    print(f\"❌ Candidate has not worked at {company_search}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhIQHDRVHRTr",
        "outputId": "2e8fc897-b86b-43a6-c2bd-e06a93aa1c21"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Candidate has technical skill: Python\n",
            "❌ Candidate has not worked at Google\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I am saving all parsed resume data into a JSON file\n",
        "# This can be later used for batch processing or building a database\n",
        "\n",
        "all_results = []  # List to store multiple resume results\n",
        "all_results.append(result)\n",
        "\n",
        "# Save results in a single JSON file\n",
        "with open(\"all_parsed_resumes.json\", \"w\") as f:\n",
        "    json.dump(all_results, f, indent=4)\n",
        "\n",
        "print(\"✅ All parsed resumes saved in 'all_parsed_resumes.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1wUkkwmHTfO",
        "outputId": "d853e4a4-31c2-4955-8be6-52c9f1563630"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All parsed resumes saved in 'all_parsed_resumes.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Here I am showing simple statistics from parsed resumes\n",
        "# For example, total resumes parsed, common skills found, etc.\n",
        "\n",
        "print(f\"Total resumes processed: {len(all_results)}\")\n",
        "\n",
        "# Count of each technical skill\n",
        "skill_count = {}\n",
        "for res in all_results:\n",
        "    for skill in res['skills']['technical']:\n",
        "        skill_count[skill] = skill_count.get(skill, 0) + 1\n",
        "\n",
        "print(\"📊 Technical Skills Frequency:\")\n",
        "for skill, count in skill_count.items():\n",
        "    print(f\"{skill}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSdPab8KHV4-",
        "outputId": "ffad1127-6e15-4810-9d6d-196837ff2d02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total resumes processed: 1\n",
            "📊 Technical Skills Frequency:\n",
            "Python: 1\n",
            "Java: 1\n",
            "Javascript: 1\n",
            "C++: 1\n",
            "Php: 1\n",
            "Go: 1\n",
            "Html: 1\n",
            "Css: 1\n",
            "Flask: 1\n",
            "Sql: 1\n",
            "Mysql: 1\n",
            "Aws: 1\n",
            "Git: 1\n",
            "Machine Learning: 1\n",
            "Pandas: 1\n"
          ]
        }
      ]
    }
  ]
}
